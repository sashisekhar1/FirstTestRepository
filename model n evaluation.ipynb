{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fad5a67-f830-45ea-bb02-394309437ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # import library to read data into dataframe\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "import numpy as np # import numpy library\n",
    "import re # import library for regular expression\n",
    "import random # library for random number generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2847a389-cb4d-4cec-a28d-b1b5deac5fcd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyodide'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyodide\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhttp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pyfetch\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdownload\u001b[39m(url, filename):\n\u001b[0;32m      4\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m pyfetch(url)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyodide'"
     ]
    }
   ],
   "source": [
    "from pyodide.http import pyfetch\n",
    " \n",
    "async def download(url, filename):\n",
    "    response = await pyfetch(url)\n",
    "    if response.status == 200:\n",
    "        with open(filename, \"wb\") as f:\n",
    "            f.write(await response.bytes())\n",
    " \n",
    "path = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DS0103EN-SkillsNetwork/labs/Module%202/recipes.csv\"\n",
    " \n",
    "#you will need to download the dataset; if you are running locally, please comment out the following \n",
    "await download(path, \"recipes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24135a3f-8a3c-47f5-b05f-5ccc7c79d42e",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'recipes.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m recipes \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecipes.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData read into dataframe!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mE:\\Anaconda\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mE:\\Anaconda\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mE:\\Anaconda\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32mE:\\Anaconda\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mE:\\Anaconda\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'recipes.csv'"
     ]
    }
   ],
   "source": [
    "recipes = pd.read_csv(\"recipes.csv\")\n",
    "\n",
    "print(\"Data read into dataframe!\") # takes about 30 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "738f2a27-7b5d-4b3c-8711-c807d5fa23cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "read_csv() missing 1 required positional argument: 'filepath_or_buffer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m pd\u001b[38;5;241m.\u001b[39mread_csv()\n",
      "\u001b[1;31mTypeError\u001b[0m: read_csv() missing 1 required positional argument: 'filepath_or_buffer'"
     ]
    }
   ],
   "source": [
    "pd.read_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "090ec286-66ff-4b78-9882-78d808bf54a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 110) (4181704740.py, line 110)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[6], line 110\u001b[1;36m\u001b[0m\n\u001b[1;33m    Here, you are creating a decision tree for the recipes for just some of the Asian (Korean, Japanese, Chinese, Thai) and Indian cuisines. The reason this action is because the decision tree does not run well when the data is biased towards one cuisine or a group of cuisines, such as in this case, American cuisines. We can exclude the American cuisines from our analysis or just build decision trees for different subsets of the data. Let's go with the second solution.\u001b[0m\n\u001b[1;37m                                                                                                                                                                                                                                                                                                                                                                                                                                                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unterminated string literal (detected at line 110)\n"
     ]
    }
   ],
   "source": [
    "Using this notebook:\n",
    "To run any of the following cells of code, you can type Shift + Enter to excute the code in a cell.\n",
    "\n",
    "First, download the library and dependencies that you will need to run this lab.\n",
    "\n",
    "code 1\n",
    "import pandas as pd # import library to read data into dataframe\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "import numpy as np # import numpy library\n",
    "import re # import library for regular expression\n",
    "import random # library for random number generation\n",
    "\n",
    "code 2\n",
    "from pyodide.http import pyfetch\n",
    " \n",
    "async def download(url, filename):\n",
    "    response = await pyfetch(url)\n",
    "    if response.status == 200:\n",
    "        with open(filename, \"wb\") as f:\n",
    "            f.write(await response.bytes())\n",
    " \n",
    "path = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DS0103EN-SkillsNetwork/labs/Module%202/recipes.csv\"\n",
    " \n",
    "#you will need to download the dataset; if you are running locally, please comment out the following \n",
    "await download(path, \"recipes.csv\")\n",
    "\n",
    "\n",
    "We already placed the data on an IBM server for your convenience, so download the data from the server and read the data into a dataframe called recipes.\n",
    "\n",
    "code 3\n",
    "recipes = pd.read_csv(\"recipes.csv\")\n",
    "\n",
    "print(\"Data read into dataframe!\") # takes about 30 seconds\n",
    "\n",
    "\n",
    "\n",
    "Next, repeat the preprocessing steps that you implemented in the lab From Understanding to Preparation to prepare the data for modeling. For more details on preparing the data, please refer to the Lab From Understanding to Preparation.\n",
    "\n",
    "code 4\n",
    "# fix name of the column displaying the cuisine\n",
    "column_names = recipes.columns.values\n",
    "column_names[0] = \"cuisine\"\n",
    "recipes.columns = column_names\n",
    "\n",
    "# convert cuisine names to lower case\n",
    "recipes[\"cuisine\"] = recipes[\"cuisine\"].str.lower()\n",
    "\n",
    "# make the cuisine names consistent\n",
    "recipes.loc[recipes[\"cuisine\"] == \"austria\", \"cuisine\"] = \"austrian\"\n",
    "recipes.loc[recipes[\"cuisine\"] == \"belgium\", \"cuisine\"] = \"belgian\"\n",
    "recipes.loc[recipes[\"cuisine\"] == \"china\", \"cuisine\"] = \"chinese\"\n",
    "recipes.loc[recipes[\"cuisine\"] == \"canada\", \"cuisine\"] = \"canadian\"\n",
    "recipes.loc[recipes[\"cuisine\"] == \"netherlands\", \"cuisine\"] = \"dutch\"\n",
    "recipes.loc[recipes[\"cuisine\"] == \"france\", \"cuisine\"] = \"french\"\n",
    "recipes.loc[recipes[\"cuisine\"] == \"germany\", \"cuisine\"] = \"german\"\n",
    "recipes.loc[recipes[\"cuisine\"] == \"india\", \"cuisine\"] = \"indian\"\n",
    "recipes.loc[recipes[\"cuisine\"] == \"indonesia\", \"cuisine\"] = \"indonesian\"\n",
    "recipes.loc[recipes[\"cuisine\"] == \"iran\", \"cuisine\"] = \"iranian\"\n",
    "recipes.loc[recipes[\"cuisine\"] == \"italy\", \"cuisine\"] = \"italian\"\n",
    "recipes.loc[recipes[\"cuisine\"] == \"japan\", \"cuisine\"] = \"japanese\"\n",
    "recipes.loc[recipes[\"cuisine\"] == \"israel\", \"cuisine\"] = \"jewish\"\n",
    "recipes.loc[recipes[\"cuisine\"] == \"korea\", \"cuisine\"] = \"korean\"\n",
    "recipes.loc[recipes[\"cuisine\"] == \"lebanon\", \"cuisine\"] = \"lebanese\"\n",
    "recipes.loc[recipes[\"cuisine\"] == \"malaysia\", \"cuisine\"] = \"malaysian\"\n",
    "recipes.loc[recipes[\"cuisine\"] == \"mexico\", \"cuisine\"] = \"mexican\"\n",
    "recipes.loc[recipes[\"cuisine\"] == \"pakistan\", \"cuisine\"] = \"pakistani\"\n",
    "recipes.loc[recipes[\"cuisine\"] == \"philippines\", \"cuisine\"] = \"philippine\"\n",
    "recipes.loc[recipes[\"cuisine\"] == \"scandinavia\", \"cuisine\"] = \"scandinavian\"\n",
    "recipes.loc[recipes[\"cuisine\"] == \"spain\", \"cuisine\"] = \"spanish_portuguese\"\n",
    "recipes.loc[recipes[\"cuisine\"] == \"portugal\", \"cuisine\"] = \"spanish_portuguese\"\n",
    "recipes.loc[recipes[\"cuisine\"] == \"switzerland\", \"cuisine\"] = \"swiss\"\n",
    "recipes.loc[recipes[\"cuisine\"] == \"thailand\", \"cuisine\"] = \"thai\"\n",
    "recipes.loc[recipes[\"cuisine\"] == \"turkey\", \"cuisine\"] = \"turkish\"\n",
    "recipes.loc[recipes[\"cuisine\"] == \"vietnam\", \"cuisine\"] = \"vietnamese\"\n",
    "recipes.loc[recipes[\"cuisine\"] == \"uk-and-ireland\", \"cuisine\"] = \"uk-and-irish\"\n",
    "recipes.loc[recipes[\"cuisine\"] == \"irish\", \"cuisine\"] = \"uk-and-irish\"\n",
    "\n",
    "\n",
    "# remove data for cuisines with < 50 recipes:\n",
    "recipes_counts = recipes[\"cuisine\"].value_counts()\n",
    "cuisines_indices = recipes_counts > 50\n",
    "\n",
    "cuisines_to_keep = list(np.array(recipes_counts.index.values)[np.array(cuisines_indices)])\n",
    "recipes = recipes.loc[recipes[\"cuisine\"].isin(cuisines_to_keep)]\n",
    "\n",
    "# convert all Yes's to 1's and the No's to 0's\n",
    "recipes = recipes.replace(to_replace=\"Yes\", value=1)\n",
    "recipes = recipes.replace(to_replace=\"No\", value=0)\n",
    "\n",
    "\n",
    "Data Modeling\n",
    "https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DS0103EN-SkillsNetwork/labs/Module%204/images/flowchart_data_modeling.png\n",
    "\n",
    "Now download and install more libraries and dependencies to build decision trees.\n",
    "\n",
    "code 5\n",
    "acy_score, confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# If running locally, you can try using the graphviz library but we'll use sklearn's plot_tree method\n",
    "# !conda install python-graphviz --yes\n",
    "# from sklearn.tree import export_graphviz\n",
    "\n",
    "\n",
    "Check the data again!\n",
    "recipes.head()\n",
    "\n",
    "[bamboo_tree] Only Asian and Indian Cuisines\n",
    "Here, you are creating a decision tree for the recipes for just some of the Asian (Korean, Japanese, Chinese, Thai) and Indian cuisines. The reason this action is because the decision tree does not run well when the data is biased towards one cuisine or a group of cuisines, such as in this case, American cuisines. We can exclude the American cuisines from our analysis or just build decision trees for different subsets of the data. Let's go with the second solution.\n",
    "\n",
    "Let's build our decision tree using the data pertaining to the Asian and Indian cuisines and name our decision tree bamboo_tree.\n",
    "\n",
    "\n",
    "code 6\n",
    "# select subset of cuisines\n",
    "asian_indian_recipes = recipes[recipes.cuisine.isin([\"korean\", \"japanese\", \"chinese\", \"thai\", \"indian\"])]\n",
    "cuisines = asian_indian_recipes[\"cuisine\"]\n",
    "ingredients = asian_indian_recipes.iloc[:,1:]\n",
    "\n",
    "bamboo_tree = tree.DecisionTreeClassifier(max_depth=3)\n",
    "bamboo_tree.fit(ingredients, cuisines)\n",
    "\n",
    "print(\"Decision tree model saved to bamboo_tree!\")\n",
    "\n",
    "\n",
    "Let's plot the decision tree and examine what the decision tree looks like.\n",
    "\n",
    "code 7\n",
    "# if you're using the graphviz library, you can run these lines of code. Otherwise, this is configured to use plot_tree from sklearn\n",
    "# export_graphviz(bamboo_tree,\n",
    "#                 feature_names=list(ingredients.columns.values),\n",
    "#                 out_file=\"bamboo_tree.dot\",\n",
    "#                 class_names=np.unique(cuisines),\n",
    "#                 filled=True,\n",
    "#                 node_ids=True,\n",
    "#                 special_characters=True,\n",
    "#                 impurity=False,\n",
    "#                 label=\"all\",\n",
    "#                 leaves_parallel=False)\n",
    "\n",
    "# with open(\"bamboo_tree.dot\") as bamboo_tree_image:\n",
    "#     bamboo_tree_graph = bamboo_tree_image.read()\n",
    "# graphviz.Source(bamboo_tree_graph)\n",
    "\n",
    "plt.figure(figsize=(40,20))  # customize according to the size of your tree\n",
    "_ = tree.plot_tree(bamboo_tree,\n",
    "                   feature_names = list(ingredients.columns.values),\n",
    "                   class_names=np.unique(cuisines),filled=True,\n",
    "                   node_ids=True,\n",
    "                   impurity=False,\n",
    "                   label=\"all\",\n",
    "                   fontsize=20, rounded = True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "The decision tree learned:\n",
    "\n",
    "If a recipe contains cumin and fish and no yoghurt, then it is most likely a Thai recipe.\n",
    "If a recipe contains cumin but no fish and no soy_sauce, then it is most likely an Indian recipe.\n",
    "You can analyze the remaining branches of the tree to come up with similar rules for determining the cuisine of different recipes.\n",
    "\n",
    "Feel free to select another subset of cuisines and build a decision tree of their recipes. You can select some European cuisines and build decision trees to explore the ingredients that differentiate those cuisines.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Model Evaluation\n",
    "https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DS0103EN-SkillsNetwork/labs/Module%204/images/flowchart_evaluation.png\n",
    "\n",
    "Next, to evaluate our model of Asian and Indian cuisines, you will split the data set into a training set and a test set. You will build the decision tree using the training set. Then, you will test the model on the test set and compare the cuisines that the model predicts to the actual cuisines.\n",
    "\n",
    "First create a new dataframe using only the data pertaining to the Asian and the Indian cuisines, and let's call the new dataframe bamboo.\n",
    "\n",
    "code 8\n",
    "bamboo = recipes[recipes.cuisine.isin([\"korean\", \"japanese\", \"chinese\", \"thai\", \"indian\"])]\n",
    "\n",
    "Let's remove 30 recipes from each cuisine to use as the test set, and let's name this test set bamboo_test.\n",
    "\n",
    "code 9\n",
    "# set sample size\n",
    "sample_n = 30\n",
    "\n",
    "Next, create a dataframe containing 30 recipes from each cuisine, selected randomly.\n",
    "\n",
    "code 10\n",
    "# take 30 recipes from each cuisine\n",
    "random.seed(1234) # set random seed\n",
    "bamboo_test = bamboo.groupby(\"cuisine\", group_keys=False).apply(lambda x: x.sample(sample_n))\n",
    "\n",
    "bamboo_test_ingredients = bamboo_test.iloc[:,1:] # ingredients\n",
    "bamboo_test_cuisines = bamboo_test[\"cuisine\"] # corresponding cuisines or labels\n",
    "\n",
    "\n",
    "Check that there are 30 recipes for each cuisine.\n",
    "\n",
    "# check that we have 30 recipes from each cuisine\n",
    "bamboo_test[\"cuisine\"].value_counts()\n",
    "\n",
    "Now create the training set by removing the test set from the bamboo data set, and name the training set bamboo_train.\n",
    "\n",
    "bamboo_test_index = bamboo.index.isin(bamboo_test.index)\n",
    "bamboo_train = bamboo[~bamboo_test_index]\n",
    "\n",
    "bamboo_train_ingredients = bamboo_train.iloc[:,1:] # ingredients\n",
    "bamboo_train_cuisines = bamboo_train[\"cuisine\"] # corresponding cuisines or labels\n",
    "\n",
    "Verify that there are 30 fewer recipes now for each cuisine.\n",
    "\n",
    "bamboo_train[\"cuisine\"].value_counts()\n",
    "\n",
    "Let's build the decision tree using the training set, **bamboo_train**, and name the generated tree **bamboo_train_tree** for prediction.\n",
    "\n",
    "\n",
    "bamboo_train_tree = tree.DecisionTreeClassifier(max_depth=15)\n",
    "bamboo_train_tree.fit(bamboo_train_ingredients, bamboo_train_cuisines)\n",
    "\n",
    "print(\"Decision tree model saved to bamboo_train_tree!\")\n",
    "\n",
    "Let's plot the decision tree and explore it.\n",
    "\n",
    "\n",
    "# export_graphviz(bamboo_train_tree,\n",
    "#                 feature_names=list(bamboo_train_ingredients.columns.values),\n",
    "#                 out_file=\"bamboo_train_tree.dot\",\n",
    "#                 class_names=np.unique(bamboo_train_cuisines),\n",
    "#                 filled=True,\n",
    "#                 node_ids=True,\n",
    "#                 special_characters=True,\n",
    "#                 impurity=False,\n",
    "#                 label=\"all\",\n",
    "#                 leaves_parallel=False)\n",
    "\n",
    "# with open(\"bamboo_train_tree.dot\") as bamboo_train_tree_image:\n",
    "#     bamboo_train_tree_graph = bamboo_train_tree_image.read()\n",
    "# graphviz.Source(bamboo_train_tree_graph)\n",
    "\n",
    "plt.figure(figsize=(40,20))  # customize according to the size of your tree\n",
    "_ = tree.plot_tree(bamboo_train_tree,\n",
    "                   feature_names=list(bamboo_train_ingredients.columns.values),\n",
    "                   class_names=np.unique(bamboo_train_cuisines),\n",
    "                   filled=True,\n",
    "                   node_ids=True,\n",
    "                   impurity=False,\n",
    "                   label=\"all\",\n",
    "                   fontsize=10, rounded = True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "Now that you defined our tree to be deeper, more decision nodes are generated.\n",
    "\n",
    "Now let's test our model on the test data.\n",
    "\n",
    "\n",
    "bamboo_pred_cuisines = bamboo_train_tree.predict(bamboo_test_ingre\n",
    "\n",
    "To quantify how well the decision tree is able to determine the cuisine of each recipe correctly, we will create a confusion matrix which presents a nice summary on how many recipes from each cuisine are correctly classified. It also sheds some light on what cuisines are being confused with what other cuisines.\n",
    "\n",
    "\n",
    "So let's go ahead and create the confusion matrix for how well the decision tree is able to correctly classify the recipes in bamboo_test.\n",
    "\n",
    "\n",
    "\n",
    "test_cuisines = np.unique(bamboo_test_cuisines)\n",
    "bamboo_confusion_matrix = confusion_matrix(bamboo_test_cuisines, bamboo_pred_cuisines, labels = test_cuisines)\n",
    "title = 'Bamboo Confusion Matrix'\n",
    "cmap = plt.cm.Blues\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "bamboo_confusion_matrix = (\n",
    "    bamboo_confusion_matrix.astype('float') / bamboo_confusion_matrix.sum(axis=1)[:, np.newaxis]\n",
    "    ) * 100\n",
    "\n",
    "plt.imshow(bamboo_confusion_matrix, interpolation='nearest', cmap=cmap)\n",
    "plt.title(title)\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(test_cuisines))\n",
    "plt.xticks(tick_marks, test_cuisines)\n",
    "plt.yticks(tick_marks, test_cuisines)\n",
    "\n",
    "fmt = '.2f'\n",
    "thresh = bamboo_confusion_matrix.max() / 2.\n",
    "for i, j in itertools.product(range(bamboo_confusion_matrix.shape[0]), range(bamboo_confusion_matrix.shape[1])):\n",
    "    plt.text(j, i, format(bamboo_confusion_matrix[i, j], fmt),\n",
    "             horizontalalignment=\"center\",\n",
    "             color=\"white\" if bamboo_confusion_matrix[i, j] > thresh else \"black\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "After running the above code, you should get a confusion matrix similar to the following:\n",
    "\n",
    "https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DS0103EN-SkillsNetwork/labs/Module%204/images/lab4_fig6_confusion_matrix.png\n",
    "\n",
    "\n",
    "\n",
    "The rows represent the actual cuisines from the dataset and the columns represent the predicted ones. Each row should sum to 100%. According to this confusion matrix, we make the following observations:\n",
    "\n",
    "Using the first row in the confusion matrix, 60% of the Chinese recipes in bamboo_test were correctly classified by our decision tree whereas 37% of the Chinese recipes were misclassified as Korean and 3% were misclassified as Indian.\n",
    "\n",
    "Using the Indian row, 77% of the Indian recipes in bamboo_test were correctly classified by our decision tree and 3% of the Indian recipes were misclassified as Chinese and 13% were misclassified as Korean and 7% were misclassified as Thai.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a06ef9a-a9f9-4341-ad75-729bb4b938f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
